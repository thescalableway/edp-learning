{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dlt - configuration & secrets\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "There are several things we need to configure in a dlt project:\n",
    "- the tool itself\n",
    "- project-level default values\n",
    "- secrets\n",
    "\n",
    "### A note on the scope of this notebook\n",
    "\n",
    "While dlt supports [various config providers](https://dlthub.com/docs/general-usage/credentials/setup#available-config-providers), to keep things brief, in this tutorial we will be focusing on only one of the available approaches: config files.\n",
    "\n",
    "### Tool configuration\n",
    "\n",
    "`dlt` is typically configured in the `config.toml` file. Here you can specify project-level configuration of internal settings such as loading behavior or memory optimizations.\n",
    "\n",
    "### Project-level default values\n",
    "\n",
    "We can specify these in the `config.toml` file as well. Project defaults can include things such as reusable destination configuration; for example, if using a data lake as the staging layer, you can specify the data lake config (such as the bucket and path to the base directory).\n",
    "\n",
    "### Secrets\n",
    "\n",
    "While the best-practice way of specifying secrets is via [vaults](https://dlthub.com/docs/general-usage/credentials/setup#vaults), this feature is not yet provided out-of-the-box by dlt and requires a custom implementation. Therefore, for the purposes of this tutorial, we will be using the `secrets.toml` file.\n",
    "\n",
    "### Config/secret injection mechanism\n",
    "\n",
    "But, how do we use config/secrets in our pipelines?\n",
    "\n",
    "For this purpose, `dlt` has a config injection mechanism, used to \"magically\" fetch config/secrets values for us by using two \"magic\" variables, `dlt.secrets.value` or `dlt.config.value`. Don't worry if this is confusing right now; will show this in practice in our pipelines later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "The best place to store common, non-secret configuration, is the `config.toml` file.\n",
    "\n",
    "Recall when we first introduced `dlt` in the previous notebook? We mentioned that it consists of three stages (extract, normalize, and load), and that it stored extracted and normalized data locally before loading it into the destination.\n",
    "\n",
    "The details of this process depend on used destination. By default, for DuckDB, `dlt` stores the final data (called a \"load package\") as compressed INSERT files, which makes it hard to inspect if you ever wanted to debug the data before it's loaded into the destination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up any existing files.\n",
    "!rm -rf ~/.dlt/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse an example pipeline from intro notebook.\n",
    "\n",
    "import dlt\n",
    "\n",
    "people = [\n",
    "    {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "    {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "    {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": \"Poland\"},\n",
    "]\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.extract(people, table_name=\"person\")\n",
    "load_info = pipeline.normalize()\n",
    "\n",
    "print(load_info)\n",
    "\n",
    "load_file_path = next(job.file_path for job in load_info.load_packages[0].jobs[\"new_jobs\"] if job.job_file_info.table_name == \"person\")\n",
    "print()\n",
    "print(f\"Load file path: {load_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "This is what the load file looks like on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!gzip -dc $load_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use dlt's config to change that behavior. We will make `dlt` store load packages as `parquet` files instead of `insert_values`.\n",
    "\n",
    "Uncomment the `[normalize]` and `loader_file_format` lines in `.dlt/config.toml` (lines 7-8). This setting changes the load file format to `parquet`. Next, reload the notebook and execute all the cells above except the last command (`gzip`).\n",
    "\n",
    "This is what the load file looks now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install required dependency.\n",
    "!uv add pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(load_file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Secrets\n",
    "\n",
    "In this section, we will show dlt's config injection mechanism in practice. Without further ado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse an example pipeline from intro notebook.\n",
    "\n",
    "import dlt\n",
    "\n",
    "\n",
    "# NOTE: the last person's country is taken from the `secrets.toml` file.\n",
    "@dlt.resource(table_name=\"person\")\n",
    "def people(secret_country: str = dlt.secrets.value):\n",
    "    yield [\n",
    "        {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "        {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "        {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": secret_country},\n",
    "    ]\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(people)\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"select * from mydata.person where name = 'Rafal Brzoska';\" | duckdb dummy_source_to_duckdb.duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "`dlt` automatically fetched the value for the `secret_country` parameter using its [injection mechanism](https://dlthub.com/docs/general-usage/credentials/setup#naming-convention).\n",
    "\n",
    "In this case, we've specified our secret in the `<pipeline_name>` config section in `secrets.toml`. Alternatively, dlt offers [other ways of providing the secret config](https://dlthub.com/docs/general-usage/credentials/setup#naming-convention). One common way is to specify destination credentials in the following way:\n",
    "\n",
    "```toml\n",
    "[destinations.destination_name.credentials]\n",
    "some_credential = \"some_value\"\n",
    "```\n",
    "\n",
    "For example:\n",
    "\n",
    "```toml\n",
    "[destination.bigquery.credentials]\n",
    "project_id = \"project_id\" # please set me up!\n",
    "private_key = \"private_key\" # please set me up!\n",
    "client_email = \"client_email\" # please set me up!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Preview: vault integration\n",
    "\n",
    "As a bonus, let's show how you could go about using your company's vault (eg. Google Cloud Secret Manager) for storing secrets used in `dlt` pipelines.\n",
    "\n",
    "Essentially, we need to implement a utility function for fetching secrets, and then use it whenever we need to fetch a secret value within the resource/pipeline code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse an example pipeline from intro notebook.\n",
    "\n",
    "import dlt\n",
    "\n",
    "\n",
    "def get_secret_from_gcsm(secret_name: str):\n",
    "    \"\"\"\n",
    "    We'd normally fetch the value of the user-specified secret from the vault here.\n",
    "\n",
    "    In the case of GCSM, OAuth can be used to authenticate the machine with Google\n",
    "    Cloud before executing this code. In such case, we don't need to provide GCSM\n",
    "    credentials anywhere in dlt.\n",
    "\n",
    "    In case of some other vaults where we need to pass credentials to the vault\n",
    "    itself, we can store those in an environment variable in our production machines,\n",
    "    and utilize `dlt.secrets.values` with the environment variables config provider\n",
    "    (eg. credentials: GcpServiceAccountCredentials = dlt.secrets.value) together\n",
    "    with the with_config() decorator to use these vault credentials securely.\n",
    "    \"\"\"\n",
    "    return f\"{secret_name}'s secret_value\"\n",
    "\n",
    "\n",
    "# NOTE: the last person's country is taken from the `secrets.toml` file.\n",
    "@dlt.resource(table_name=\"person\")\n",
    "def people(secret_country: str | None = None):\n",
    "    secret_country = secret_country or get_secret_from_gcsm(\"secret_country\")\n",
    "    yield [\n",
    "        {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "        {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "        {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": secret_country},\n",
    "    ]\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(people)\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"select * from mydata.person where name = 'Rafal Brzoska';\" | duckdb dummy_source_to_duckdb.duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we've learned:\n",
    "\n",
    "- what are the ways to configure things in `dlt` project: `dlt` itself, common pipeline configuration, and secrets\n",
    "- how to use `config.toml` and `secrets.toml` and how dlt's injection mechanism works\n",
    "- (bonus) how we could use an external vault such as Google Cloud Secret Manager for storing our secrets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
