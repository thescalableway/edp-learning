{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Loading and Write Dispositions\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this section, we will discover how to use `dlt` effectively by loading only new/modified data using two dlt features in tandem: write dispositions and incremental loading.\n",
    "\n",
    "\n",
    "### ELT patterns\n",
    "\n",
    "There are two ideal data source types, in terms of efficiency:\n",
    "- an immutable source (eg. logs), from which we're able to extract only the new records\n",
    "\n",
    "  In this case, we're able to use incremental loading with the `append` strategy to load data in the most efficient way.\n",
    "- a mutable source (eg. a database), but one from which we're able to extract new and modified records\n",
    "\n",
    "    In this case, we can use the `merge` write disposition.\n",
    "\n",
    "The diagram below describes the most optimal ELT strategy given how we're able to extract data from a data source.\n",
    "\n",
    "![](https://thescalableway.com/img/uT145YgjSn-960.webp)\n",
    "\n",
    "Credit: https://thescalableway.com/blog/dlt-and-prefect-a-great-combo-for-streamlined-data-ingestion-pipelines/#efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on the scope of this notebook\n",
    "\n",
    "While `dlt` supports handling of deleted records in the `merge` write disposition, doing so depends on upstream source managing these records in a specific way (we need a column that indicates whether a record has been deleted). This is not a common practice and implementing such scenarios typically requires data engineering work at the data generation level (eg. collaborating with database admins), making this an advanced scenario, and so we will not cover it in this notebook.\n",
    "\n",
    "For now, assume that in the case records are deleted, a full refresh must be performed.\n",
    "\n",
    "For more information, see [dlt documentation](https://dlthub.com/docs/general-usage/incremental-loading#delete-records)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write Dispositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Incremental loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!rm -f _clickstream_last_id.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "from secrets import choice, randbelow\n",
    "from typing import Any\n",
    "\n",
    "import dlt\n",
    "from dlt.pipeline.pipeline import Pipeline\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "n_users = 10\n",
    "# n_clicks = 100000\n",
    "n_clicks = 10000\n",
    "\n",
    "def person() -> Generator[dict[str, Any], None, None]:\n",
    "    \"\"\"Simulate data from a source.\n",
    "\n",
    "    We keep the first row static, while rows 2 and 3 are \"updated\" each time the\n",
    "    function is called.\n",
    "\n",
    "    We also showcase the usage of `cursor.last_value`, which could be used to filter\n",
    "    only new data at the extract stage (eg, by passing it to a filtering parameter\n",
    "    such as `since` in a REST API).\n",
    "\n",
    "    For more information on this usage, see\n",
    "    https://dlthub.com/docs/general-usage/incremental-loading#incremental-loading-with-a-cursor-field.\n",
    "\n",
    "    The `cursor` variable is injected by the `edp_resource()` decorator.\n",
    "    \"\"\"\n",
    "    ids = range(n_users)\n",
    "    # Simulate updating two random rows.\n",
    "    ids_to_update = sample(ids, 2)\n",
    "    for _id in ids:\n",
    "        yield {\n",
    "            \"id\": _id,\n",
    "            \"name\": fake.name(),\n",
    "            \"country\": choice([\"USA\", \"China\", \"Poland\"]),\n",
    "            \"updated_at\": datetime.now(UTC)\n",
    "            if _id in ids_to_update\n",
    "            else datetime(2024, 1, 1, 0, 0, 0, 0, UTC),\n",
    "        }\n",
    "\n",
    "\n",
    "def clickstream() -> Generator[dict[str, Any], None, None]:\n",
    "    \"\"\"Simulate clickstream data.\"\"\"\n",
    "    # Keep a cursor so we can simulate incremental loading.\n",
    "    cursor_file_path = Path(\"_clickstream_last_id.txt\")\n",
    "    if cursor_file_path.exists():\n",
    "        with cursor_file_path.open() as f:\n",
    "            last_id = int(f.read())\n",
    "    else:\n",
    "        last_id = 0\n",
    "\n",
    "    pages = [\"/home\", \"/about\", \"/contact\", \"/pricing\", \"/blog\"]\n",
    "    yield [\n",
    "        {\n",
    "            \"id\": _,\n",
    "            \"user_id\": randbelow(n_users),\n",
    "            \"timestamp\": fake.date_time_between(start_date=\"-1m\", end_date=\"now\"),\n",
    "            \"page\": choice(pages),\n",
    "        }\n",
    "        for _ in range(last_id + 1, last_id + 1 + n_clicks)\n",
    "    ]\n",
    "\n",
    "    new_last_id = last_id + n_clicks\n",
    "    with cursor_file_path.open(\"w\") as f:\n",
    "        f.write(str(new_last_id))\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=dlt.destinations.duckdb(\"incremental.duckdb\"),\n",
    "    dataset_name=\"bronze\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run the pipeline again; notice duplicated data\n",
    "- TODO: show modified script, execute, show both tables (merge & append)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information regarding integrating `dlt` with Prefect, see [our article](https://thescalableway.com/blog/dlt-and-prefect-a-great-combo-for-streamlined-data-ingestion-pipelines) on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
