{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging & Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "A good sign of a production-ready library or tool is how well it handles operational aspects, such as:\n",
    "- how good the errors are (error handling, error messages)\n",
    "- how good the logging is\n",
    "- does it have any tracing capabilities\n",
    "\n",
    "While we've already seen some of the information generated by `dlt` when executing our pipelines (the `LoadInfo` object), we haven't yet discussed how to configure production-grade logging or tracing. In this notebook, we will cover these topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logging\n",
    "\n",
    "Logging is a critical aspect of any software system. It provides a way to track the execution of the system, and it's essential for debugging and monitoring.\n",
    "\n",
    "`dlt` has a few configurations and ways of working related to logging. Let's look at them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Logging Configuration\n",
    "\n",
    "To configure logging, set the `runtime.log_level` config in `config.toml`. Typically, setting this to `INFO` is a good starting point. Let's do that now (uncomment line 3 in `config.toml`) and see how it affects the output of a pipeline:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "ðŸ’¡ We've removed load info from the output so that only the logging output is displayed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "people = [\n",
    "    {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "    {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "    {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": \"Poland\"},\n",
    "]\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(people, table_name=\"person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Progress\n",
    "\n",
    "Often times, it's very helpful to see how many resources (such as RAM and CPU) the pipeline is consuming.\n",
    "\n",
    "While it's possible to track such information at the infrastructure level, depending on the setup, it might not always be possible to dig down to the pipeline level. Additioanally, it often requires setting up additional infrastructure tooling, such as Grafana.\n",
    "\n",
    "Luckily, `dlt` provides very a simple way to track this information per-pipeline. All we have to do is specify the `progress` parameter in our pipeline definition. Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "people = [\n",
    "    {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "    {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "    {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": \"Poland\"},\n",
    "]\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    "    progress=\"log\",  # NOTE: Add this line to see resource consumption in the logs.\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(people, table_name=\"person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: the near-infinite customizability of `dlt`\n",
    "\n",
    "As in many other places, here too we can dig deeper and configure the minute details of how the progress is displayed. Besides being able to choose different display methods (eg. using Pyton's popular `tqdm` progress bar library), we can also better control the behavior of each method by using the relevant `dlt.progress` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.common import logger as dlt_logger\n",
    "import logging\n",
    "\n",
    "\n",
    "people = [\n",
    "    {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "    {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "    {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": \"Poland\"},\n",
    "]\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    "    # NOTE: We change \"log\" (which uses default parameters to dlt.progress.log())\n",
    "    # to `dlt.progress.log()` with our custom parameters.\n",
    "    # NOTE 2: `log_period` is the *maximum* time between progress updates. In other\n",
    "    # words, if a step takes shorter, dlt will ignore this value and still emit a\n",
    "    # progress update.\n",
    "    progress=dlt.progress.log(log_period=5, logger=dlt_logger.LOGGER, log_level=logging.DEBUG),\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(people, table_name=\"person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we lost all the progress logs? That's because we've set the logging level of those messages to `DEBUG`, and by default, Jupyter only displays `INFO` and higher logs. Go back to the pipeline and set logging level to `ERROR` instead, and you will see progress logs appear again.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "ðŸ’¡ Software engineering tip\n",
    "    <br>\n",
    "\n",
    "   Using log levels allows us to better filter out different kinds of information, thus simplifying the debugging process.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're not in a production setting, let's turn off the verbose logging before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Pipeline run metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides tracking the execution of the pipeline itself, `dlt` is also produces metadata about each pipeline run. This is the metadata that we've been displaying throughout the lessons so far, stored in the `LoadInfo` object returned by `pipeline.run()`.\n",
    "\n",
    "Since this information can also be useful for tracking or debugging purposes, it's a good idea to store it somewhere. One way to do this would be to simply log this information as part of the pipeline script. However, this has the downside of having to then parse logs in order to extract pipeine run statistics, such as  duration of the `normalize` step, etc.\n",
    "\n",
    "What we can do instead is to load this information into a separate table in the destination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "people = [\n",
    "    {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "    {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "    {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": \"Poland\"},\n",
    "]\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(people, table_name=\"person\")\n",
    "pipeline.run([load_info], table_name=\"_load_info\")\n",
    "\n",
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT * FROM information_schema.tables\") as cursor:\n",
    "        data = cursor.df()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that a few new tables have been created in the destination. Engineers can now use these tables for example to build analytics on pipeline execution*.\n",
    "\n",
    "*Read on to learn how this could be extended with additional metadata from tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tracing\n",
    "\n",
    "Tracing is a way to track the execution of a system across multiple components. It's especially useful in distributed systems, where a single request might be processed by multiple services.\n",
    "\n",
    "Since this is an advanced concept, we will only briefly show how tracing information can be stored using `dlt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run([pipeline.last_trace], table_name=\"_trace\")\n",
    "\n",
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT * FROM information_schema.tables\") as cursor:\n",
    "        data = cursor.df()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we've learned:\n",
    "\n",
    "- How to configure logging in `dlt`\n",
    "- How to track progress of a pipeline and its resource use\n",
    "- How to work with pipeline run metadata\n",
    "- How to store tracing information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
