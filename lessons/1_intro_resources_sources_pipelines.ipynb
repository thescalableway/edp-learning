{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Introduction to dlt: Resources, Sources, and Pipelines\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### What is dlt?\n",
    "\n",
    "[dlt](https://dlthub.com/) is a Python data ingestion framework enabling data engineers to define connectors and pipelines as code. It offers a rich set of features for building best-practice pipelines and supports both built-in and custom connectors built with regular Python code.\n",
    "\n",
    "For more information, see the [official docs](https://dlthub.com/docs/intro#what-is-dlt).\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "dlt ingests data in three stages: extract, normalize, and load. The extract stage downloads source data to disk. The normalize stage applies light transformations to the data, such as column renaming or datetime parsing. The load stage loads the data into the destination system.\n",
    "\n",
    "![how does dlt work diagram](https://dlthub.com/docs/assets/images/dlt-onepager-c61255330e30060ca8f2fa6d7b73b600.png)\n",
    "Credit: [dlt documentation](https://dlthub.com/docs/reference/explainers/how-dlt-works)\n",
    "\n",
    "## 2. Hello, dlt\n",
    "\n",
    "Let's jump in and see dlt in action!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "# Sample data.\n",
    "people = [\n",
    "    {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "    {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "    {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": \"Poland\"},\n",
    "]\n",
    "\n",
    "# Set pipeline name, destination, and dataset name.\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(people, table_name=\"person\")\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "Let's see what was loaded into the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"select table_catalog, table_schema, table_name from information_schema.tables;\" | duckdb dummy_source_to_duckdb.duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"select * from mydata.person;\" | duckdb dummy_source_to_duckdb.duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, we will learn more about the features and ways of working with dlt as we build gradually more complex pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sources, resources, and pipelines\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "- learn about the three key dlt concepts\n",
    "- configure a source and two resources\n",
    "- briefly showcase destinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Resources\n",
    "\n",
    "Resources represent the data that flows through a dlt pipeline. They allow us to use various dlt funcionalities such as incremental loading and specify some of the ELT configuration, which we would be unable to do if we worked with raw data.\n",
    "\n",
    "In the script below, we use `dlt.resource()` in order to specify the name of the target table when defining the resource, rather than at pipeline runtime (`pipeline.run()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "# We now describe the data source as a dlt resource rather than a Python list.\n",
    "@dlt.resource(table_name=\"person\")\n",
    "def people():\n",
    "    yield [\n",
    "        {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "        {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "        {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": \"Poland\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(people)\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sources\n",
    "\n",
    "Sources are groups of resources. They allow us to define the source of the data and the resources that will be loaded from that source. For example, a source could be an SQL database, while a resource would be a table in that database.\n",
    "\n",
    "dlt offers several built-in standard sources such as databases, REST APIs, or cloud storage. We can also define custom sources by using the `dlt.source()` decorator.\n",
    "\n",
    "In the script below, we define a custom source with two resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def dummy_data():\n",
    "    @dlt.resource(table_name=\"person\")\n",
    "    def people():\n",
    "        yield [\n",
    "            {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "            {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "            {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": \"Poland\"},\n",
    "        ]\n",
    "\n",
    "    @dlt.resource(table_name=\"country\")\n",
    "    def countries():\n",
    "        yield [\n",
    "            {\"id\": \"1\", \"name\": \"USA\", \"population\": 331449281},\n",
    "            {\"id\": \"2\", \"name\": \"China\", \"population\": 1444216107},\n",
    "            {\"id\": \"3\", \"name\": \"Poland\", \"population\": 37846611},\n",
    "        ]\n",
    "\n",
    "    # NOTE: We need to return the resources here.\n",
    "    return people, countries\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # NOTE: We still provide resources as the data, since `dummy_data()` returns\n",
    "    # the two resources.\n",
    "    load_info = pipeline.run(data=dummy_data())\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A more real-life example\n",
    "\n",
    "Just to show you how easy this is with any built-in dlt source as well, let's sidestep and quickly load some data from a production MySQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install required dependencies for the MySQL connector.\n",
    "!uv add dlt --extra sql_database\n",
    "!uv add pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: executing this might take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "\n",
    "# NOTE: without .with_resources(), we'd be replicating the entire database, which is\n",
    "# too resource-intensive for this tutorial.\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\"\n",
    ").with_resources(\"family\", \"genome\")\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_example\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(source)\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"select * from sql_data.family limit 3;\" | duckdb sql_database_example.duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Just like that, we loaded real-life data from a MySQL database into our local DuckDB instance*.\n",
    "\n",
    "*Note that this is a public database and so we didn't have to specify any credentials. We'll learn about those later in the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pipelines\n",
    "\n",
    "In `dlt`, a pipeline describes the flow of data from resource(s) to a destination. Each pipeline loads resources to a single destination.\n",
    "\n",
    "Pipelines can be reused to ingest different resources each run. For example, we can have one “Postgres to S3” pipeline, but ingest each Postgres table separately due to different scheduling or configuration needs.\n",
    "\n",
    "A pipeline definition contains pipeline- or pipeline run-specific destination configuration, as well as settings for the load phase of the ingestion. Under the hood, a pipeline run (`pipeline.run()`) executes each pipeline step: extract (`pipeline.extract()`), normalize (`pipeline.normalize()`), and load (`pipeline.load()`).\n",
    "\n",
    "Let's use this knowledge to better control our destination configuration - in this case, we'll control the name of the DuckDB database where the data will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def dummy_data():\n",
    "    @dlt.resource(table_name=\"person\")\n",
    "    def people():\n",
    "        yield [\n",
    "            {\"id\": \"1\", \"name\": \"Warren Buffet\", \"country\": \"USA\"},\n",
    "            {\"id\": \"2\", \"name\": \"Jack Ma\", \"country\": \"China\"},\n",
    "            {\"id\": \"3\", \"name\": \"Rafal Brzoska\", \"country\": \"Poland\"},\n",
    "        ]\n",
    "\n",
    "    @dlt.resource(table_name=\"country\")\n",
    "    def countries():\n",
    "        yield [\n",
    "            {\"id\": \"1\", \"name\": \"USA\", \"population\": 331449281},\n",
    "            {\"id\": \"2\", \"name\": \"China\", \"population\": 1444216107},\n",
    "            {\"id\": \"3\", \"name\": \"Poland\", \"population\": 37846611},\n",
    "        ]\n",
    "\n",
    "    return people, countries\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=dlt.destinations.duckdb(\"db.duckdb\"),  # NOTE: we renamed the db here.\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_info = pipeline.run(data=dummy_data())\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting data in the destination\n",
    "\n",
    "`dlt` offers two main built-in ways to inspect the data in the destination, SQL client and datasets.\n",
    "\n",
    "##### SQL client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install required dependencies for the SQL client.\n",
    "!uv add pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT * FROM mydata.person\") as cursor:\n",
    "        data = cursor.df()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pipeline.dataset(dataset_type=\"default\")\n",
    "dataset.country.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercise 1\n",
    "\n",
    "Let's now use all of the knowledge we've gained to build a pipeline that loads data from two CSV files into a DuckDB database:\n",
    "\n",
    "- define a source, `csvs`, that will contain CSV file resources\n",
    "- use provided two util functions to define two resources, `iris` and `wine`\n",
    "- define a pipeline that will load the resources into a `bronze` schema in a `csvs.duckdb` DuckdDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here.\n",
    "import dlt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_iris():\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "    col_names = [\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\", \"Class\"]\n",
    "    return pd.read_csv(url, names=col_names).to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "def read_wine():\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "    col_names = [\n",
    "        \"fixed_acidity\",\n",
    "        \"volatile_acidity\",\n",
    "        \"citric_acid\",\n",
    "        \"residual_sugar\",\n",
    "        \"chlorides\",\n",
    "        \"free_sulfur_dioxide\",\n",
    "        \"total_sulfur_dioxide\",\n",
    "        \"density\",\n",
    "        \"pH\",\n",
    "        \"sulphates\",\n",
    "        \"alcohol\",\n",
    "        \"quality\",\n",
    "        \"color\",\n",
    "    ]\n",
    "    return pd.read_csv(url, names=col_names).to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# Define the resource.\n",
    "...\n",
    "\n",
    "# Define the pipeline.\n",
    "...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline and print load info.\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "For the solution to this exercise, see the solutions notebook (`1b_solutions.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, we should be able to query the data now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"select * from bronze.iris limit 3;\" | duckdb csvs.duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"select * from bronze.wine limit 3;\" | duckdb csvs.duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "In this lesson, we've:\n",
    "\n",
    "- learned and got a feel for what dlt is\n",
    "- learned about its fundamental concepts: sources, resources, and pipelines\n",
    "- loaded some fake and actual data from various sources (a Python object, MySQL database, CSV files) into a local DuckDB database\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
