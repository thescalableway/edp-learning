{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Loading and Write Dispositions\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this section, we will discover how to use `dlt` effectively by loading only new/modified data using two dlt features in tandem: write dispositions and incremental loading.\n",
    "\n",
    "### ELT patterns\n",
    "\n",
    "There are two ideal data source types, in terms of efficiency:\n",
    "- an immutable source (eg. logs), from which we're able to extract only the new records\n",
    "\n",
    "  In this case, we're able to use incremental loading with the `append` strategy to load data in the most efficient way.\n",
    "- a mutable source (eg. a database), but one from which we're able to extract new and modified records\n",
    "\n",
    "    In this case, we can use the `merge` write disposition.\n",
    "\n",
    "The diagram below describes the most optimal ELT strategy given how we're able to extract data from a data source.\n",
    "\n",
    "![](https://thescalableway.com/img/uT145YgjSn-960.webp)\n",
    "\n",
    "Credit: https://thescalableway.com/blog/dlt-and-prefect-a-great-combo-for-streamlined-data-ingestion-pipelines/#efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on deletes\n",
    "\n",
    "While `dlt` supports handling of deleted records in the `merge` write disposition, doing so depends on upstream source managing these records in a specific way (we need a column that indicates whether a record has been deleted). This is not a common practice and implementing such scenarios typically requires data engineering work at the data generation level (eg. collaborating with database admins), making this an advanced scenario, and so we will not cover it in this notebook.\n",
    "\n",
    "For now, assume that in the case records are deleted, a full refresh must be performed.\n",
    "\n",
    "For more information, see [dlt documentation](https://dlthub.com/docs/general-usage/incremental-loading#delete-records)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Incremental loading\n",
    "\n",
    "### What is incremental loading\n",
    "\n",
    "Incremental loading lets us extract and normalize only the new or modified records from a data source. It relies on a column that indicates when a record was last updated, which is typically a timestamp column such as `modified_at`, or a sequence number. This column is called a \"cursor column\", and the last processed value of this column is called a \"cursor value\".\n",
    "\n",
    "### A note on how dbt stores pipeline state\n",
    "\n",
    "Let's get back to the first pipeline we've ever executed. Remember those odd `_dlt` tables that were created automatically?\n",
    "\n",
    "| table_catalog          | table_schema | table_name              |\n",
    "|------------------------|--------------|-------------------------|\n",
    "| dummy_source_to_duckdb  | mydata       | person                  |\n",
    "| dummy_source_to_duckdb  | mydata       | **_dlt_loads**          |\n",
    "| dummy_source_to_duckdb  | mydata       | **_dlt_pipeline_state** |\n",
    "| dummy_source_to_duckdb  | mydata       | **_dlt_version**        |\n",
    "\n",
    "This is one of the instances where they're useful. Specifically, for incremental data loads, dlt tracks each incremental pipeline's cursor value in these tables, which allows each pipeline run to resume from where the previous run stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental loading in action\n",
    "\n",
    "In this pipeline, we will generate some fake incremental data. The dataset contains of 10 records, 2 of which are updated each time the pipeline is executed.  Our goal is to only load the updated records in subsequent executions of the pipeline.\n",
    "\n",
    "To start off, let's see what happens if we have a changing dataset, but don't configure incremental loading in `dlt`:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "⚠️ Please esure that load file format is set to parquet (as described in lesson 2) before proceeding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "from datetime import UTC, datetime\n",
    "from random import sample\n",
    "from secrets import choice\n",
    "from typing import Any\n",
    "\n",
    "import dlt\n",
    "from faker import Faker\n",
    "\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "n_users = 10\n",
    "\n",
    "\n",
    "def person() -> Generator[dict[str, Any], None, None]:\n",
    "    \"\"\"Simulate data from a source.\n",
    "\n",
    "    We keep the first row static, while rows 2 and 3 are \"updated\" each time the\n",
    "    function is called.\n",
    "\n",
    "    We also showcase the usage of `cursor.last_value`, which could be used to filter\n",
    "    only new data at the extract stage (eg, by passing it to a filtering parameter\n",
    "    such as `since` in a REST API).\n",
    "\n",
    "    For more information on this usage, see\n",
    "    https://dlthub.com/docs/general-usage/incremental-loading#incremental-loading-with-a-cursor-field.\n",
    "\n",
    "    The `cursor` variable is injected by the `edp_resource()` decorator.\n",
    "    \"\"\"\n",
    "    ids = range(n_users)\n",
    "    # Simulate updating two random rows.\n",
    "    ids_to_update = sample(ids, 2)\n",
    "    for _id in ids:\n",
    "        yield {\n",
    "            \"id\": _id,\n",
    "            \"name\": fake.name(),\n",
    "            \"country\": choice([\"USA\", \"China\", \"Poland\"]),\n",
    "            \"updated_at\": datetime.now(UTC)\n",
    "            if _id in ids_to_update\n",
    "            else datetime(2024, 1, 1, 0, 0, 0, 0, UTC),\n",
    "        }\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=dlt.destinations.duckdb(\"incremental.duckdb\"),\n",
    "    dataset_name=\"bronze\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_info = pipeline.extract(data=person)\n",
    "    load_info = pipeline.normalize()\n",
    "\n",
    "print(load_info)\n",
    "\n",
    "normalized_data_path = next(\n",
    "    job.file_path\n",
    "    for job in load_info.load_packages[0].jobs[\"new_jobs\"]\n",
    "    if job.job_file_info.table_name == \"person\"\n",
    ")\n",
    "print()\n",
    "print(f\"Normalized data path: {normalized_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install required dependency.\n",
    "!uv add pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_parquet(normalized_data_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, re-execute the pipeline and check what the normalized data looks like.\n",
    "\n",
    "That's right, we see 10 rows again, most of which were updated back in January 2024! This means that we're re-extracting and re-normalizing the same data over and over again.\n",
    "\n",
    "Let's fix that by using the power of dlt resources.\n",
    "\n",
    "Execute this pipline **twice** to see that on the second run, only the updated records are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "from typing import Any\n",
    "\n",
    "import dlt\n",
    "from faker import Faker\n",
    "\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "n_users = 10\n",
    "\n",
    "\n",
    "@dlt.resource()  # NOTE: Make `person` a dlt resource\n",
    "def person() -> Generator[dict[str, Any], None, None]:\n",
    "    \"\"\"Simulate data from a source.\n",
    "\n",
    "    We keep the first row static, while rows 2 and 3 are \"updated\" each time the\n",
    "    function is called.\n",
    "\n",
    "    We also showcase the usage of `cursor.last_value`, which could be used to filter\n",
    "    only new data at the extract stage (eg, by passing it to a filtering parameter\n",
    "    such as `since` in a REST API).\n",
    "\n",
    "    For more information on this usage, see\n",
    "    https://dlthub.com/docs/general-usage/incremental-loading#incremental-loading-with-a-cursor-field.\n",
    "\n",
    "    The `cursor` variable is injected by the `edp_resource()` decorator.\n",
    "    \"\"\"\n",
    "    ids = range(n_users)\n",
    "    # Simulate updating two random rows.\n",
    "    ids_to_update = sample(ids, 2)\n",
    "    for _id in ids:\n",
    "        yield {\n",
    "            \"id\": _id,\n",
    "            \"name\": fake.name(),\n",
    "            \"country\": choice([\"USA\", \"China\", \"Poland\"]),\n",
    "            \"updated_at\": datetime.now(UTC)\n",
    "            if _id in ids_to_update\n",
    "            else datetime(2024, 1, 1, 0, 0, 0, 0, UTC),\n",
    "        }\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=dlt.destinations.duckdb(\"incremental.duckdb\"),\n",
    "    dataset_name=\"bronze\",\n",
    ")\n",
    "\n",
    "# NOTE: Specify the cursor column.\n",
    "person.apply_hints(incremental=dlt.sources.incremental(\"updated_at\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_info = pipeline.extract(data=person)\n",
    "    load_info = pipeline.normalize()\n",
    "\n",
    "print(load_info)\n",
    "\n",
    "normalized_data_path = next(\n",
    "    job.file_path\n",
    "    for job in load_info.load_packages[0].jobs[\"new_jobs\"]\n",
    "    if job.job_file_info.table_name == \"person\"\n",
    ")\n",
    "print()\n",
    "print(f\"Normalized data path: {normalized_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_parquet(normalized_data_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, how do we tell `dlt` to not for example append these rows in the destination, but to update them instead? Enter **write dispositions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write Dispositions\n",
    "\n",
    "Write dispositions tell `dlt` how data should be loaded into the destination. The three main ones are: \"append\", \"replace\", and \"merge\". In our case, we're interested in the \"merge\" disposition.\n",
    "\n",
    "Below is a cheat sheet on which disposition to use in which scenario:\n",
    "\n",
    "![dlt write dispositions decision tree](https://storage.googleapis.com/dlt-blog-images/flowchart_for_scd2.png)\n",
    "\n",
    "Credit: [dlt documentation](https://dlthub.com/docs/general-usage/incremental-loading#two-simple-questions-determine-the-write-disposition-you-use)\n",
    "\n",
    "Let's first see what happens if we extract data incrementally, but don't specify any write disposition. Execute the pipeline below **twice**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "from typing import Any\n",
    "\n",
    "import dlt\n",
    "from faker import Faker\n",
    "\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "n_users = 10\n",
    "\n",
    "\n",
    "@dlt.resource()\n",
    "def person() -> Generator[dict[str, Any], None, None]:\n",
    "    \"\"\"Simulate data from a source.\n",
    "\n",
    "    We keep the first row static, while rows 2 and 3 are \"updated\" each time the\n",
    "    function is called.\n",
    "\n",
    "    We also showcase the usage of `cursor.last_value`, which could be used to filter\n",
    "    only new data at the extract stage (eg, by passing it to a filtering parameter\n",
    "    such as `since` in a REST API).\n",
    "\n",
    "    For more information on this usage, see\n",
    "    https://dlthub.com/docs/general-usage/incremental-loading#incremental-loading-with-a-cursor-field.\n",
    "\n",
    "    The `cursor` variable is injected by the `edp_resource()` decorator.\n",
    "    \"\"\"\n",
    "    ids = range(n_users)\n",
    "    # Simulate updating two random rows.\n",
    "    ids_to_update = sample(ids, 2)\n",
    "    for _id in ids:\n",
    "        yield {\n",
    "            \"id\": _id,\n",
    "            \"name\": fake.name(),\n",
    "            \"country\": choice([\"USA\", \"China\", \"Poland\"]),\n",
    "            \"updated_at\": datetime.now(UTC)\n",
    "            if _id in ids_to_update\n",
    "            else datetime(2024, 1, 1, 0, 0, 0, 0, UTC),\n",
    "        }\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=dlt.destinations.duckdb(\"incremental.duckdb\"),\n",
    "    dataset_name=\"bronze\",\n",
    ")\n",
    "\n",
    "\n",
    "person.apply_hints(incremental=dlt.sources.incremental(\"updated_at\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # NOTE: Chance this to run() to execute the full EL process.\n",
    "    load_info = pipeline.run(person)\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT * FROM bronze.person\") as cursor:\n",
    "        data = cursor.df()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up.\n",
    "!rm -f incremental.duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happened?\n",
    "\n",
    "We correctly extracted only the two modified rows, but we appended them to the destination table, resulting in two dupicate rows (assuming `id` if our unique identifier).\n",
    "\n",
    "#### Using the `merge` write disposition\n",
    "Let's fix that by specifying the `merge` write disposition*.\n",
    "\n",
    "*Note that for the merge disposition to work, we also need to specify the primary key of our resource. You can read more about each write disposition in the [dlt documentation](https://dlthub.com/docs/general-usage/incremental-loading).\n",
    "\n",
    "Execute below pipeline twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "from typing import Any\n",
    "\n",
    "import dlt\n",
    "from faker import Faker\n",
    "\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "n_users = 10\n",
    "\n",
    "\n",
    "@dlt.resource()\n",
    "def person() -> Generator[dict[str, Any], None, None]:\n",
    "    \"\"\"Simulate data from a source.\n",
    "\n",
    "    We keep the first row static, while rows 2 and 3 are \"updated\" each time the\n",
    "    function is called.\n",
    "\n",
    "    We also showcase the usage of `cursor.last_value`, which could be used to filter\n",
    "    only new data at the extract stage (eg, by passing it to a filtering parameter\n",
    "    such as `since` in a REST API).\n",
    "\n",
    "    For more information on this usage, see\n",
    "    https://dlthub.com/docs/general-usage/incremental-loading#incremental-loading-with-a-cursor-field.\n",
    "\n",
    "    The `cursor` variable is injected by the `edp_resource()` decorator.\n",
    "    \"\"\"\n",
    "    ids = range(n_users)\n",
    "    # Simulate updating two random rows.\n",
    "    ids_to_update = sample(ids, 2)\n",
    "    for _id in ids:\n",
    "        yield {\n",
    "            \"id\": _id,\n",
    "            \"name\": fake.name(),\n",
    "            \"country\": choice([\"USA\", \"China\", \"Poland\"]),\n",
    "            \"updated_at\": datetime.now(UTC)\n",
    "            if _id in ids_to_update\n",
    "            else datetime(2024, 1, 1, 0, 0, 0, 0, UTC),\n",
    "        }\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"dummy_source_to_duckdb\",\n",
    "    destination=dlt.destinations.duckdb(\"incremental.duckdb\"),\n",
    "    dataset_name=\"bronze\",\n",
    ")\n",
    "\n",
    "person.apply_hints(incremental=dlt.sources.incremental(\"updated_at\"))\n",
    "person.apply_hints(primary_key=\"id\")  # NOTE: Specify the primary key.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # NOTE: Add `merge` write disposition.\n",
    "    load_info = pipeline.run(person, write_disposition=\"merge\")\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we're **updating** the rows that have changed instead of **appending** them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT * FROM bronze.person\") as cursor:\n",
    "        data = cursor.df()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 10 records, and we now have different `updated_at` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus - using `_dlt_load_id`\n",
    "\n",
    "We can also utilize `_dlt_load_id` to inspect or debug incremental loads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\n",
    "        \"SELECT _dlt_load_id, count(*) as records FROM bronze.person GROUP BY _dlt_load_id\"\n",
    "    ) as cursor:\n",
    "        data = cursor.df()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column is created automatically by `dlt` (the behavior is configurable in the `config.toml` file).\n",
    "\n",
    "**NOTE** This column will also come in handy when working with dlt-loaded datasets in the Transform layer, eg. when creating `dbt` models, as it will allow as to easily define incremental models in a standardized way and without any additional setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we've learned:\n",
    "- what is incremental loading, its related concepts (curor column and cursor value), and how it fits into the Extract and Load process\n",
    "- how to use incremental loading to extract only new or modified records from a data source\n",
    "- how to use write dispositions to control how data is loaded into the destination\n",
    "- how to use `_dlt_load_id` to inspect or debug incremental loads\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
